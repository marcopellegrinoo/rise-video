{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 11:53:03.555032: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-05 11:53:04.739439: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-05 11:53:08.480897: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-03-05 11:53:08.484102: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-03-05 11:53:08.936447: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-05 11:53:10.498617: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-05 11:53:10.805545: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-05 11:53:32.190711: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105\n",
      "105\n",
      "105\n",
      "[<keras.src.engine.functional.Functional object at 0x7f088e7bbcd0>, <keras.src.engine.functional.Functional object at 0x7f0887bdba00>, <keras.src.engine.functional.Functional object at 0x7f0887b4cfa0>, <keras.src.engine.functional.Functional object at 0x7f08879a6a70>, <keras.src.engine.functional.Functional object at 0x7f0887a2b700>, <keras.src.engine.functional.Functional object at 0x7f0887b7f7f0>, <keras.src.engine.functional.Functional object at 0x7f08878c6b30>, <keras.src.engine.functional.Functional object at 0x7f08879f3910>, <keras.src.engine.functional.Functional object at 0x7f08877c1030>, <keras.src.engine.functional.Functional object at 0x7f08877ffa00>]\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"rise_st_multiplicative_norm_zero_cineca.ipynb\n",
    "\n",
    "Automatically generated by Colab.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/1Vxe74LFV5mUpC-IZp8BSjnQnJbXwjopw\n",
    "\n",
    "### ***Cineca***\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.models import load_model\n",
    "from keras import activations\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from keras.models import load_model\n",
    "from skimage.transform import resize\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\"\"\"\n",
    "##### ***Data & Black-Box***\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# IMPORTO I DATI PER VOTTIGNASCO\n",
    "\n",
    "\n",
    "# Ottieni il percorso effettivo da una variabile d'ambiente\n",
    "work_path = os.environ['WORK']  # Ottieni il valore della variabile d'ambiente WORK\n",
    "v_test_OHE_path = os.path.join(work_path, \"Water_Resources/rise-video/data/Vottignasco/Vottignasco_00425010001_test_month_OHE.npy\")\n",
    "v_test_image_path = os.path.join(work_path, \"Water_Resources/rise-video/data/Vottignasco/Vottignasco_00425010001_test_normalized_image_sequences.npy\")\n",
    "v_test_target_dates_path = os.path.join(work_path, \"Water_Resources/rise-video/data/Vottignasco/Vottignasco_00425010001_test_target_dates.npy\")\n",
    "\n",
    "# Carica l'array numpy dai file\n",
    "vottignasco_test_OHE    = np.load(v_test_OHE_path)\n",
    "vottignasco_test_image  = np.load(v_test_image_path)\n",
    "vottignasco_test_dates  = np.load(v_test_target_dates_path)\n",
    "\n",
    "\n",
    "print(len(vottignasco_test_dates))\n",
    "print(len(vottignasco_test_image))\n",
    "print(len(vottignasco_test_OHE))\n",
    "\n",
    "#print(vottingasco_test_OHE[0], \"\\n\")\n",
    "#print(vottignasco_test_image[0][0], \"\\n\")\n",
    "\n",
    "# \"\"\"##### ***Black Boxes***\"\"\"\n",
    "\n",
    "\n",
    "# Se vuoi abilitare il dropout a runtime\n",
    "mc_dropout = True\n",
    "\n",
    "# Definizione della classe personalizzata doprout_custom\n",
    "class doprout_custom(tf.keras.layers.SpatialDropout1D):\n",
    "    def call(self, inputs, training=None):\n",
    "        if mc_dropout:\n",
    "            return super().call(inputs, training=True)\n",
    "        else:\n",
    "            return super().call(inputs, training=False)\n",
    "\n",
    "# Percorso della directory su Cineca\n",
    "base_dir = os.path.join(os.environ['WORK'], \"Water_Resources/rise-video/trained_models/seq2val/Vottignasco\")\n",
    "lstm_suffix = 'time_dist_LSTM'\n",
    "\n",
    "vott_lstm_models = []\n",
    "\n",
    "def extract_index(filename):\n",
    "    \"\"\"Funzione per estrarre l'indice finale dal nome del file.\"\"\"\n",
    "    return int(filename.split('_LSTM_')[-1].split('.')[0])\n",
    "\n",
    "# Trova tutti i file .keras nella cartella e li aggiunge alla lista\n",
    "for filename in os.listdir(base_dir):\n",
    "    if lstm_suffix in filename and filename.endswith(\".keras\"):\n",
    "        vott_lstm_models.append(os.path.join(base_dir, filename))\n",
    "\n",
    "# Ordina i modelli in base all'indice finale\n",
    "vott_lstm_models = sorted(vott_lstm_models, key=lambda x: extract_index(os.path.basename(x)))\n",
    "\n",
    "# Lista per i modelli caricati\n",
    "vott_lstm_models_loaded = []\n",
    "\n",
    "for i, model_lstm_path in enumerate(vott_lstm_models[:10]):  # Prendo i primi 10 modelli ordinati\n",
    "    #print(f\"Caricamento del modello LSTM {i+1}: {model_lstm_path}\")\n",
    "\n",
    "    # Carico il modello con la classe custom\n",
    "    model = load_model(model_lstm_path, custom_objects={\"doprout_custom\": doprout_custom})\n",
    "\n",
    "    # Aggiungo il modello alla lista\n",
    "    vott_lstm_models_loaded.append(model)\n",
    "\n",
    "print(vott_lstm_models_loaded)\n",
    "\n",
    "\"\"\"### ***RISE-Spatio_Temporal***\n",
    "\n",
    "#### ***Generation Masks (3D): Uniforme***\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from scipy.interpolate import RegularGridInterpolator\n",
    "from tqdm import tqdm\n",
    "\n",
    "def generate_masks_3d(N, input_size, seed=42, **kwargs):\n",
    "    \"\"\"\n",
    "    Parametri:\n",
    "    - input_size: (t, h, w) -> dimensione finale della maschera 3D (time, Height, Width)\n",
    "    \"\"\"\n",
    "    t, h, w = input_size\n",
    "    l = kwargs.get(\"l\", 8)   # Dimensione della small mask per il tempo\n",
    "    s = kwargs.get(\"s\", 8)   # Fattore per downsampling spaziale (per h e w)\n",
    "    p1 = kwargs.get(\"p1\", 0.4)  # Probabilit√† di attivazione\n",
    "\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Calcola le dimensioni spaziali della small mask usando solo h e w\n",
    "    small_s = np.ceil(np.array([h, w]) / s).astype(int)\n",
    "\n",
    "    # Genera una maschera 3D casuale di dimensione (l, small_h, small_w)\n",
    "    grid = np.random.rand(N, l, small_s[0], small_s[1]) < p1\n",
    "    grid = grid.astype('float32')\n",
    "\n",
    "    # Struttura per le maschere finali di dimensione (N, t, h, w)\n",
    "    masks = np.empty((N, t, h, w))\n",
    "\n",
    "    # Coordinate per l'interpolazione spaziale\n",
    "    grid_x = np.linspace(0, small_s[0] - 1, small_s[0])\n",
    "    grid_y = np.linspace(0, small_s[1] - 1, small_s[1])\n",
    "    grid_t = np.linspace(0, l - 1, l)\n",
    "\n",
    "    for i in tqdm(range(N), desc='Generating masks'):\n",
    "      # Crea un interpolatore per la maschera corrente\n",
    "      interpolator = RegularGridInterpolator(\n",
    "        (grid_t, grid_x, grid_y), grid[i], method='linear', bounds_error=False, fill_value=0\n",
    "      )\n",
    "      new_t = np.linspace(0, l - 1, t)\n",
    "      new_x = np.linspace(0, small_s[0] - 1, h)\n",
    "      new_y = np.linspace(0, small_s[1] - 1, w)\n",
    "      mesh_t, mesh_x, mesh_y = np.meshgrid(new_t, new_x, new_y, indexing='ij')\n",
    "      points = np.stack((mesh_t, mesh_x, mesh_y), axis=-1).reshape(-1, 3)\n",
    "      interpolated_mask = interpolator(points)\n",
    "      masks[i] = interpolated_mask.reshape(t, h, w)\n",
    "\n",
    "\n",
    "    masks = masks[~(masks == 0).all(axis=(1, 2, 3))]\n",
    "    return masks\n",
    "\n",
    "\"\"\"#### ***Application Masks***\"\"\"\n",
    "\n",
    "def multiplicative_uniform_noise_onechannel(images, masks, channel, **kwargs):\n",
    "    \"\"\"\n",
    "    Applica noise moltiplicativo continuo in 3D alle immagini sulla base delle maschere fornite.\n",
    "\n",
    "    Parametri:\n",
    "    - images: array di shape (104, 5, 8, C) -> serie temporale di immagini con canali multipli\n",
    "    - masks: array di shape (N, 104, 5, 8) -> maschere continue con valori [0, 1]\n",
    "    - channel: canale specifico su cui applicare il noise (0: Prec, 1: TMax, 2: TMin)\n",
    "    - std_zero_value: valore del disturbo da applicare dove mask = 0\n",
    "    \"\"\"\n",
    "    std_zero_value = kwargs.get(\"std_zero_value\", -0.6486319166678826)\n",
    "\n",
    "    masked = []\n",
    "\n",
    "    # Itera su tutte le N maschere generate\n",
    "    for mask in masks:\n",
    "        masked_images = copy.deepcopy(images)  # Copia profonda delle immagini originali\n",
    "\n",
    "        # Estrai solo il canale desiderato\n",
    "        channel_values = masked_images[..., channel]\n",
    "\n",
    "        # Applica la formula: v(p) + z (1-p)\n",
    "        perturbed_values = channel_values * mask + (1 - mask) * std_zero_value\n",
    "\n",
    "        # Sovrascrivi il canale con i valori perturbati\n",
    "        masked_images[..., channel] = perturbed_values\n",
    "\n",
    "        masked.append(masked_images)\n",
    "\n",
    "    return masked\n",
    "\n",
    "\"\"\"#### ***Prediction with Black-Box***\"\"\"\n",
    "\n",
    "def ensemble_predict(models, images, x3_exp):\n",
    "    # Se images √® una lista, calcoliamo la lunghezza\n",
    "    if isinstance(images, list):\n",
    "        len_x3 = len(images)\n",
    "    else:\n",
    "        len_x3 = 1\n",
    "        images = [images]  # Rendi images una lista con un solo elemento\n",
    "\n",
    "    # Conversione in tensori\n",
    "    Y_test = tf.stack([tf.convert_to_tensor(img, dtype=tf.float32) for img in images])\n",
    "    Y_test_x3 = tf.tile(tf.expand_dims(tf.convert_to_tensor(x3_exp, dtype=tf.float32), axis=0), [len_x3, 1, 1])\n",
    "\n",
    "    # Inizializza una lista per raccogliere le predizioni\n",
    "    all_preds = []\n",
    "\n",
    "    # Itera attraverso i modelli e raccogli le predizioni\n",
    "    for model in models:\n",
    "        preds = model.predict([Y_test, Y_test_x3], verbose=0)\n",
    "        all_preds.append(preds)\n",
    "\n",
    "    # Converte la lista di predizioni in un tensore di TensorFlow\n",
    "    all_preds_tensor = tf.stack(all_preds)\n",
    "\n",
    "    # Calcola la media lungo l'asse dei modelli (asse 0)\n",
    "    mean_preds = tf.reduce_mean(all_preds_tensor, axis=0)\n",
    "\n",
    "    return mean_preds.numpy()\n",
    "\n",
    "\"\"\"#### ***Saliency Map***\"\"\"\n",
    "\n",
    "def calculate_saliency_map(preds_array, masks):\n",
    "    \"\"\"\n",
    "    Calcola la mappa di salienza media data una serie di predizioni e maschere.\n",
    "\n",
    "    :param preds_array: Array di predizioni (numero di maschere x dimensioni predizione).\n",
    "    :param masks: Array di maschere (numero di maschere x dimensioni maschera).\n",
    "    :return: Mappa di salienza media.\n",
    "    \"\"\"\n",
    "    sal = []\n",
    "    for j in range(len(masks)):\n",
    "        sal_i = preds_array[j] * np.abs(masks[j])\n",
    "        sal.append(sal_i.reshape(-1, 5, 8))  # Adatta la shape secondo il formato orginiale dei frame\n",
    "\n",
    "    # Rimuovere le dimensioni extra per fare np.mean lungo axis=0. masks ha shape (N,5,8,1)\n",
    "    masks_squeezed = np.squeeze(np.abs(masks))\n",
    "    # Ora calcola la media lungo l'asse 0\n",
    "    ev_masks = np.mean(masks_squeezed, axis=0)\n",
    "    ev_masks[ev_masks == 0] = 1e-8  # Sostituiamo gli zeri con un piccolo valore\n",
    "\n",
    "    sal = (1/ev_masks) * np.mean(sal, axis=0)\n",
    "\n",
    "    return sal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#### ***Spatio_Temporal-RISE: Framework***\"\"\"\n",
    "\n",
    "# Funzione sigmoide\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def rise_st_explain_sigmoide(nr_instance, data_test_image, data_test_OHE, models, channel,\n",
    "                                   N, generate_masks_fn, seed, perturb_instance_fn, calculate_saliency_map_fn, **kwargs):\n",
    "  print(f\"############################### RISE-Temporal on Instance #{nr_instance} ###########################\")\n",
    "  instance    = copy.deepcopy(data_test_image[nr_instance])\n",
    "  x3_instance = copy.deepcopy(data_test_OHE[nr_instance])\n",
    "\n",
    "  input_size = (instance.shape[0], instance.shape[1], instance.shape[2])\n",
    "\n",
    "  masks = generate_masks_fn(N, input_size, seed, **kwargs)\n",
    "  perturbed_instances = perturb_instance_fn(instance, masks, channel)\n",
    "\n",
    "  # Predizione su Istanza Originale\n",
    "  pred_original = ensemble_predict(models, instance, x3_instance)\n",
    "  # Predizioni su Istanze Perturbate\n",
    "  preds_masked = ensemble_predict(models, perturbed_instances, x3_instance)\n",
    "\n",
    "  # Differenza tra predizione originale e perturbata\n",
    "  diff_pred = [abs(pred_original - pred_masked) for pred_masked in preds_masked]\n",
    "  weights_array = np.concatenate(diff_pred, axis=0)\n",
    "\n",
    "  # Standardizzazione (z-score normalization)\n",
    "  mean = np.mean(weights_array)\n",
    "  std = np.std(weights_array)\n",
    "  standardized_weights = (weights_array - mean) / std\n",
    "  # Applicare la sigmoide ai pesi standardizzati\n",
    "  normalized_weights_std = sigmoid(standardized_weights)\n",
    "\n",
    "  # Calcolo della mappa di salienza\n",
    "  saliency_map_i = calculate_saliency_map_fn(1-normalized_weights_std, masks)\n",
    "  print(f\"############### Processo completato. Mappa di salienza generata per Istanza #{nr_instance} ###############\")\n",
    "  return np.squeeze(saliency_map_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating masks: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 38.66it/s]\n"
     ]
    }
   ],
   "source": [
    "N = 10\n",
    "l=10 \n",
    "s=2\n",
    "p=0.5\n",
    "\n",
    "seed = 42\n",
    "\n",
    "channel = 0\n",
    "nr_instance = 0\n",
    "models = vott_lstm_models_loaded\n",
    "\n",
    "instance    = copy.deepcopy(vottignasco_test_image[nr_instance])\n",
    "x3_instance = copy.deepcopy(vottignasco_test_OHE[nr_instance])\n",
    "\n",
    "input_size = (instance.shape[0], instance.shape[1], instance.shape[2])\n",
    "\n",
    "masks = generate_masks_3d(N, input_size, seed, l=l, s=s, p1=p)\n",
    "perturbed_instances = multiplicative_uniform_noise_onechannel(instance, masks, channel)\n",
    "\n",
    "# Predizione su Istanza Originale\n",
    "pred_original = ensemble_predict(models, instance, x3_instance)\n",
    "# Predizioni su Istanze Perturbate\n",
    "preds_masked = ensemble_predict(models, perturbed_instances, x3_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.22002165]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.4628854],\n",
       "       [1.6318104],\n",
       "       [1.649078 ],\n",
       "       [1.7874655],\n",
       "       [1.1747181],\n",
       "       [1.5245184],\n",
       "       [0.93771  ],\n",
       "       [1.6932075],\n",
       "       [1.5337497],\n",
       "       [1.8104208]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.2428638],\n",
       "       [1.4117888],\n",
       "       [1.4290564],\n",
       "       [1.5674438],\n",
       "       [0.9546965],\n",
       "       [1.3044968],\n",
       "       [0.7176883],\n",
       "       [1.4731859],\n",
       "       [1.3137281],\n",
       "       [1.5903991]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.abs(pred_original - preds_masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.53425026]],\n",
       "\n",
       "       [[0.44535634]],\n",
       "\n",
       "       [[0.43657795]],\n",
       "\n",
       "       [[0.36895615]],\n",
       "\n",
       "       [[0.6908086 ]],\n",
       "\n",
       "       [[0.5012717 ]],\n",
       "\n",
       "       [[0.81136775]],\n",
       "\n",
       "       [[0.4144658 ]],\n",
       "\n",
       "       [[0.49637893]],\n",
       "\n",
       "       [[0.3582602 ]]], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_pred = [abs(pred_original - pred_masked) for pred_masked in preds_masked]\n",
    "\n",
    "kernel_width = np.percentile(diff_pred, 90)\n",
    "# Importanza vicini\n",
    "weights = np.exp(- (np.array(diff_pred) ** 2) / (kernel_width ** 2))\n",
    "\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.53425026],\n",
       "       [0.44535634],\n",
       "       [0.43657795],\n",
       "       [0.36895615],\n",
       "       [0.6908086 ],\n",
       "       [0.5012717 ],\n",
       "       [0.81136775],\n",
       "       [0.4144658 ],\n",
       "       [0.49637893],\n",
       "       [0.3582602 ]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances = np.abs(pred_original - preds_masked)\n",
    "kernel_width = np.percentile(distances, 90)\n",
    "# Importanza vicini\n",
    "weights = np.exp(- (np.array(distances) ** 2) / (kernel_width ** 2))\n",
    "\n",
    "weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
