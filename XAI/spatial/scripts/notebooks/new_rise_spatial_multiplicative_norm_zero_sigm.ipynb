{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 22:49:18.495819: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-05 22:49:20.249805: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-05 22:49:25.851423: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-03-05 22:49:25.855371: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-03-05 22:49:26.559661: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-05 22:49:29.567616: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-05 22:49:29.858074: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-05 22:50:01.101706: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105\n",
      "105\n",
      "105\n",
      "[<keras.src.engine.functional.Functional object at 0x7f15b71e6650>, <keras.src.engine.functional.Functional object at 0x7f15b07d31c0>, <keras.src.engine.functional.Functional object at 0x7f15b058fd90>, <keras.src.engine.functional.Functional object at 0x7f15b060e3b0>, <keras.src.engine.functional.Functional object at 0x7f15b04864a0>, <keras.src.engine.functional.Functional object at 0x7f15b04dd210>, <keras.src.engine.functional.Functional object at 0x7f15b0516a10>, <keras.src.engine.functional.Functional object at 0x7f15b037f310>, <keras.src.engine.functional.Functional object at 0x7f15b02351e0>, <keras.src.engine.functional.Functional object at 0x7f15b0271a80>]\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"rise_spatial_multiplicative_norm_zero_cineca.ipynb\n",
    "\n",
    "Automatically generated by Colab.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/1-4UndkKCTVbDRQZql3km54NZicEG05gb\n",
    "\n",
    "### ***Cineca***\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, activations, callbacks, models\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from keras.models import load_model\n",
    "from skimage.transform import resize\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\"\"\"\n",
    "##### ***Data & Black-Box***\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# IMPORTO I DATI PER VOTTIGNASCO\n",
    "import os\n",
    "\n",
    "# Ottieni il percorso effettivo da una variabile d'ambiente\n",
    "work_path = os.environ['WORK']  # Ottieni il valore della variabile d'ambiente WORK\n",
    "v_test_OHE_path = os.path.join(work_path, \"Water_Resources/rise-video/data/Vottignasco/Vottignasco_00425010001_test_month_OHE.npy\")\n",
    "v_test_image_path = os.path.join(work_path, \"Water_Resources/rise-video/data/Vottignasco/Vottignasco_00425010001_test_normalized_image_sequences.npy\")\n",
    "v_test_target_dates_path = os.path.join(work_path, \"Water_Resources/rise-video/data/Vottignasco/Vottignasco_00425010001_test_target_dates.npy\")\n",
    "\n",
    "# Carica l'array numpy dai file\n",
    "vottignasco_test_OHE    = np.load(v_test_OHE_path)\n",
    "vottignasco_test_image  = np.load(v_test_image_path)\n",
    "vottignasco_test_dates  = np.load(v_test_target_dates_path)\n",
    "\n",
    "\n",
    "print(len(vottignasco_test_dates))\n",
    "print(len(vottignasco_test_image))\n",
    "print(len(vottignasco_test_OHE))\n",
    "\n",
    "#print(vottingasco_test_OHE[0], \"\\n\")\n",
    "#print(vottignasco_test_image[0][0], \"\\n\")\n",
    "\n",
    "# \"\"\"##### ***Black Boxes***\"\"\n",
    "\n",
    "# Se vuoi abilitare il dropout a runtime\n",
    "mc_dropout = True\n",
    "\n",
    "# Definizione della classe personalizzata doprout_custom\n",
    "class doprout_custom(tf.keras.layers.SpatialDropout1D):\n",
    "    def call(self, inputs, training=None):\n",
    "        if mc_dropout:\n",
    "            return super().call(inputs, training=True)\n",
    "        else:\n",
    "            return super().call(inputs, training=False)\n",
    "\n",
    "# Percorso della directory su Cineca\n",
    "base_dir = os.path.join(os.environ['WORK'], \"Water_Resources/rise-video/trained_models/seq2val/Vottignasco\")\n",
    "lstm_suffix = 'time_dist_LSTM'\n",
    "\n",
    "vott_lstm_models = []\n",
    "\n",
    "def extract_index(filename):\n",
    "    \"\"\"Funzione per estrarre l'indice finale dal nome del file.\"\"\"\n",
    "    return int(filename.split('_LSTM_')[-1].split('.')[0])\n",
    "\n",
    "# Trova tutti i file .keras nella cartella e li aggiunge alla lista\n",
    "for filename in os.listdir(base_dir):\n",
    "    if lstm_suffix in filename and filename.endswith(\".keras\"):\n",
    "        vott_lstm_models.append(os.path.join(base_dir, filename))\n",
    "\n",
    "# Ordina i modelli in base all'indice finale\n",
    "vott_lstm_models = sorted(vott_lstm_models, key=lambda x: extract_index(os.path.basename(x)))\n",
    "\n",
    "# Lista per i modelli caricati\n",
    "vott_lstm_models_loaded = []\n",
    "\n",
    "for i, model_lstm_path in enumerate(vott_lstm_models[:10]):  # Prendo i primi 10 modelli ordinati\n",
    "    #print(f\"Caricamento del modello LSTM {i+1}: {model_lstm_path}\")\n",
    "\n",
    "    # Carico il modello con la classe custom\n",
    "    model = load_model(model_lstm_path, custom_objects={\"doprout_custom\": doprout_custom})\n",
    "\n",
    "    # Aggiungo il modello alla lista\n",
    "    vott_lstm_models_loaded.append(model)\n",
    "\n",
    "print(vott_lstm_models_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_rise_masks_2d(N, input_size, seed, **kwargs):\n",
    "    \"\"\"\n",
    "    Genera N maschere RISE per un'immagine di dimensioni HxW.\n",
    "\n",
    "    Parametri:\n",
    "    - N: numero di maschere\n",
    "    - input_size: (H,W) dimensioni dell'immagine originale\n",
    "    - h, w: dimensioni delle maschere a bassa risoluzione\n",
    "    - p: probabilit√† di attivazione dei pixel nella maschera binaria iniziale\n",
    "\n",
    "    Ritorna:\n",
    "    - masks: array di shape (N, H, W) contenente le maschere normalizzate.\n",
    "    \"\"\"\n",
    "\n",
    "    h  = kwargs.get(\"h\", 2)\n",
    "    w = kwargs.get(\"w\", 2)\n",
    "    p = kwargs.get(\"p\", 0.5)\n",
    "\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    masks = []\n",
    "    H,W = input_size\n",
    "    CH, CW = H // h, W // w  # Fattore di upscaling\n",
    "\n",
    "    for _ in range(N):\n",
    "        # 1. Generazione della maschera binaria iniziale (h x w)\n",
    "        small_mask = np.random.rand(h, w) < p\n",
    "\n",
    "        up_size_h = (h+1) * CH\n",
    "        up_size_w = (w+1) * CW\n",
    "\n",
    "        # 2. Upsampling bilineare alla dimensione (H + CH, W + CW\n",
    "        upsampled_mask = cv2.resize(small_mask.astype(np.float32),\n",
    "                                    (up_size_w, up_size_h), interpolation=cv2.INTER_LINEAR)\n",
    "        \n",
    "        #print(upsampled_mask.shape)\n",
    "        \n",
    "        # 3. Crop casuale della regione H x W\n",
    "        x_offset = np.random.randint(0, (up_size_h - H) + 1)\n",
    "        y_offset = np.random.randint(0, (up_size_w - W) + 1)\n",
    "        final_mask = upsampled_mask[x_offset:x_offset + H, y_offset:y_offset + W] \n",
    "\n",
    "        #print(final_mask.shape)\n",
    "\n",
    "        masks.append(final_mask)\n",
    "\n",
    "    masks = np.array(masks)  # Converte la lista in array NumPy\n",
    "    masks = masks[~(masks == 0).all(axis=(1, 2))]  # Filtro maschere vuote\n",
    "    masks = masks[~(masks == 1.0).all(axis=(1, 2))]  # Filtro maschere con tutti 1.0\n",
    "\n",
    "\n",
    "    return np.array(masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_size = (5,8)\n",
    "# h,w = 2,3\n",
    "# p   = 0.5\n",
    "\n",
    "# N = 100\n",
    "\n",
    "# seed = 42\n",
    "\n",
    "# masks = generate_rise_masks_2d(N, input_size, seed, h=h,w=w,p=p) \n",
    "\n",
    "# for nr_mask in range(0,len(masks),10):\n",
    "#     plot_frame(masks[nr_mask], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_frame(frame, cmap='viridis', title=\"Frame\"):\n",
    "#     fig, ax = plt.subplots(figsize=(7, 5))  # Dimensioni della figura\n",
    "\n",
    "#     ax.set_title(title)\n",
    "#     im = ax.imshow(frame, cmap=cmap, alpha=1.0, origin=\"lower\")  # Oggetto mappable\n",
    "\n",
    "#     # Crea una colorbar della stessa altezza della figura\n",
    "#     cbar = fig.colorbar(im, ax=ax, fraction=0.030, pad=0.04)\n",
    "\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Scelte \n",
    "# 2,2\n",
    "# 2,3\n",
    "# 2,4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiplicative_uniform_noise_onechannel(images, masks, channel, **kwargs):\n",
    "    std_zero_value = kwargs.get(\"std_zero_value\", -0.6486319166678826)\n",
    "\n",
    "    masked = []\n",
    "\n",
    "    # Itero su tutte le N maschere generate\n",
    "    for mask in masks:\n",
    "        masked_images = copy.deepcopy(images)  # Copia profonda delle immagini originali\n",
    "\n",
    "        # Applica la perturbazione solo al canale specificato\n",
    "        masked_images[..., channel] = (\n",
    "            masked_images[..., channel] * mask + (1 - mask) * std_zero_value)\n",
    "\n",
    "        masked.append(masked_images)\n",
    "\n",
    "    return masked\n",
    "\n",
    "def ensemble_predict(models, images, x3_exp, batch_size=1000):\n",
    "    # Assicuriamoci che images sia una lista\n",
    "    if not isinstance(images, list):\n",
    "        images = [images]\n",
    "\n",
    "    len_x3 = len(images)\n",
    "\n",
    "    # Convertiamo x3_exp in un tensore replicato per ogni immagine\n",
    "    x3_exp_tensor = tf.convert_to_tensor(x3_exp, dtype=tf.float32)\n",
    "\n",
    "    # Lista per raccogliere le predizioni finali\n",
    "    final_preds = []\n",
    "\n",
    "    # Processamento a batch\n",
    "    for i in range(0, len_x3, batch_size):\n",
    "        batch_images = images[i:i + batch_size]\n",
    "        batch_len = len(batch_images)\n",
    "\n",
    "        # Conversione batch in tensori\n",
    "        Y_test = tf.stack([tf.convert_to_tensor(img, dtype=tf.float32) for img in batch_images])\n",
    "        Y_test_x3 = tf.tile(tf.expand_dims(x3_exp_tensor, axis=0), [batch_len, 1, 1])\n",
    "\n",
    "        # Raccoglie le predizioni di tutti i modelli per il batch corrente\n",
    "        batch_preds = []\n",
    "\n",
    "        for model in models:\n",
    "            preds = model.predict([Y_test, Y_test_x3], verbose=0)\n",
    "            batch_preds.append(preds)\n",
    "\n",
    "        # Converte le predizioni del batch in un tensore e calcola la media\n",
    "        batch_preds_tensor = tf.stack(batch_preds)\n",
    "        mean_batch_preds = tf.reduce_mean(batch_preds_tensor, axis=0)\n",
    "\n",
    "        # Aggiunge le predizioni del batch alla lista finale\n",
    "        final_preds.extend(mean_batch_preds.numpy())\n",
    "\n",
    "    return np.array(final_preds)\n",
    "\n",
    "\"\"\"#### ***Saliency Map***\"\"\"\n",
    "\n",
    "# Modifica della funzione per calcolare la mappa di salienza introducendo anche -> E[M] cio√® il valore atteso delle Maschere\n",
    "\n",
    "def calculate_saliency_map_ev_masks(weights_array, masks):\n",
    "    \"\"\"\n",
    "    Calcola la mappa di salienza media data una serie di predizioni e maschere.\n",
    "\n",
    "    :param weights_array: Array di predizioni (pesi delle maschere).\n",
    "    :param masks: Array di maschere (numero di maschere x dimensioni maschera).\n",
    "    :return: Mappa di salienza media.\n",
    "    \"\"\"\n",
    "    sal = []\n",
    "\n",
    "    H, W = masks.shape[1], masks.shape[2]\n",
    "    for j in range(len(masks)):\n",
    "        sal_j = weights_array[j] * np.abs(masks[j])\n",
    "        sal.append(sal_j.reshape(-1, H, W))\n",
    "\n",
    "    # Rimuovere le dimensioni extra per fare np.mean lungo axis=0. masks ha shape (N,5,8,1)\n",
    "    masks_squeezed = np.squeeze(np.abs(masks))\n",
    "    # Ora calcola la media lungo l'asse 0\n",
    "    ev_masks = np.mean(masks_squeezed, axis=0)\n",
    "\n",
    "    sal = (1/ev_masks) * np.mean(sal, axis=0)  # aggiunta della frazione 1/valore_atteso(maschere)\n",
    "\n",
    "    return np.squeeze(sal)\n",
    "\n",
    "\"\"\"#### ***Spatial-RISE: Framework***\"\"\"\n",
    "\n",
    "# Funzione sigmoide\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def rise_spatial_explain_sigmoide(nr_instance, data_test_image, data_test_OHE, models, channel,\n",
    "                                  N, generate_masks_fn, seed, perturb_instance_fn, calculate_saliency_map_fn, **kwargs):\n",
    "  print(f\"############################### RISE-Spatial on Instance #{nr_instance} ###########################\")\n",
    "  instance    = copy.deepcopy(data_test_image[nr_instance])\n",
    "  x3_instance = copy.deepcopy(data_test_OHE[nr_instance])\n",
    "\n",
    "  input_size = (instance.shape[1], instance.shape[2])\n",
    "\n",
    "  masks = generate_masks_fn(N, input_size, seed, **kwargs)\n",
    "  perturbed_instances = perturb_instance_fn(instance, masks, channel)\n",
    "\n",
    "  # Predizione su Istanza Originale\n",
    "  pred_original = ensemble_predict(models, instance, x3_instance)\n",
    "  # Predizioni su Istanze Perturbate\n",
    "  preds_masked = ensemble_predict(models, perturbed_instances, x3_instance)\n",
    "\n",
    "  # Differenza tra predizione originale e perturbata\n",
    "  diff_pred = [abs(pred_original - pred_masked) for pred_masked in preds_masked]\n",
    "  weights_array = np.concatenate(diff_pred, axis=0)\n",
    "\n",
    "  # Standardizzazione (z-score normalization)\n",
    "  mean = np.mean(weights_array)\n",
    "  std = np.std(weights_array)\n",
    "  standardized_weights = (weights_array - mean) / std\n",
    "  # Applicare la sigmoide ai pesi standardizzati\n",
    "  normalized_weights_std = sigmoid(standardized_weights)\n",
    "\n",
    "  # Calcolo della mappa di salienza\n",
    "  saliency_map_i = calculate_saliency_map_fn(1-normalized_weights_std, masks)\n",
    "  print(f\"############### Processo completato. Mappa di salienza generata per Istanza #{nr_instance} ###############\")\n",
    "  return np.squeeze(saliency_map_i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#### ***Evaluation Metrics***\"\"\"\n",
    "\n",
    "\n",
    "def plot_insertion_curve(total_errors, auc, title=\"Insertion Metric Curve\"):\n",
    "    \"\"\"\n",
    "    Plotta la curva di metrica di insertion con l'errore medio quadratico.\n",
    "\n",
    "    :param total_errors: Lista dei valori dell'errore per ogni frazione di pixel inseriti.\n",
    "    :param auc: Valore dell'Area Under Curve (AUC) calcolato.\n",
    "    :param title: Titolo del grafico (default: \"Insertion Metric Curve\").\n",
    "    \"\"\"\n",
    "\n",
    "    # Nuovo asse X normalizzato tra 0 e 1\n",
    "    x = np.linspace(0, 1, len(total_errors))\n",
    "\n",
    "    # Plot della curva dell'errore e dell'area sotto la curva (AUC)\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    plt.plot(x, total_errors, marker='o', linestyle='-', color='blue')\n",
    "\n",
    "    # Pallini blu sui punti della curva\n",
    "    plt.scatter(x, total_errors, color='blue', zorder=3)\n",
    "\n",
    "    # Area sotto la curva\n",
    "    plt.fill_between(x, total_errors, color='skyblue', alpha=0.4)\n",
    "\n",
    "    # Testo \"Error curve\" in alto a dx con font pi√π piccolo\n",
    "    plt.legend(['Error curve'], loc='upper right', fontsize=9)\n",
    "\n",
    "    # Testo AUC appena sotto \"Error curve\"\n",
    "    plt.text(x[-1] - 0.02, max(total_errors) * 0.9,\n",
    "             f'AUC: {auc:.2f}',\n",
    "             horizontalalignment='right',\n",
    "             fontsize=8,\n",
    "             bbox=dict(facecolor='white', alpha=0.5))\n",
    "\n",
    "    # Etichette assi\n",
    "    plt.xlabel('Fraction of pixels inserted')\n",
    "    plt.ylabel('Mean Squared Error')\n",
    "\n",
    "    # Titolo del grafico\n",
    "    plt.title(title)\n",
    "\n",
    "    # Mostra il grafico\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_deletion_curve(total_errors, auc, title=\"Deletion Metric Curve\"):\n",
    "    \"\"\"\n",
    "    Plotta la curva della metrica di deletion con l'errore medio quadratico.\n",
    "\n",
    "    :param total_errors: Lista dei valori dell'errore per ogni frazione di pixel rimossi.\n",
    "    :param auc: Valore dell'Area Under Curve (AUC) calcolato.\n",
    "    :param title: Titolo del grafico (default: \"Deletion Metric Curve\").\n",
    "    \"\"\"\n",
    "\n",
    "    # Normalizzazione dell'asse X tra 0 e 1\n",
    "    x = np.linspace(0, 1, len(total_errors))\n",
    "\n",
    "    # Creazione del plot\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.plot(x, total_errors, marker='o', linestyle='-', color='red')\n",
    "\n",
    "    # Pallini rossi sui punti della curva\n",
    "    plt.scatter(x, total_errors, color='red', zorder=3)\n",
    "\n",
    "    # Area sotto la curva\n",
    "    plt.fill_between(x, total_errors, color='lightcoral', alpha=0.4)\n",
    "\n",
    "    # Testo \"Error curve\" in alto a sx con font pi√π piccolo\n",
    "    plt.legend(['Error curve'], loc='upper left', fontsize=9)\n",
    "\n",
    "    # Testo AUC appena sotto \"Error curve\"\n",
    "    plt.text(x[0] + 0.01, max(total_errors) * 0.88,\n",
    "             f'AUC: {auc:.2f}',\n",
    "             horizontalalignment='left',\n",
    "             fontsize=8,\n",
    "             bbox=dict(facecolor='white', alpha=0.5))\n",
    "\n",
    "    # Etichette degli assi\n",
    "    plt.xlabel('Fraction of pixels removed')\n",
    "    plt.ylabel('Mean Squared Error')\n",
    "\n",
    "    # Titolo del grafico\n",
    "    plt.title(title)\n",
    "\n",
    "    # Mostra il grafico\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def calculate_auc(x, y):\n",
    "    \"\"\"\n",
    "    Calcola l'area sotto la curva (AUC) utilizzando il metodo del trapezio.\n",
    "\n",
    "    :param x: Valori dell'asse x (frazione dei pixel/frame inseriti).\n",
    "    :param y: Valori dell'asse y (errori calcolati).\n",
    "    :return: Area sotto la curva.\n",
    "    \"\"\"\n",
    "    return np.trapz(y, x)\n",
    "\n",
    "\n",
    "# Restituisce n-esimo percentile dei pixel pi√π importanti delle mappa di salienza data in input\n",
    "def get_top_n_pixels(saliency_map, n):\n",
    "    # Appiattisci la mappa di salienza\n",
    "    flat_saliency = saliency_map.flatten()\n",
    "    # Ordina gli indici degli elementi in ordine decrescente di salienza\n",
    "    sorted_indices = np.argsort(flat_saliency)[::-1]\n",
    "\n",
    "    # Calcola il numero di colonne della mappa di salienza\n",
    "    num_cols = saliency_map.shape[1]\n",
    "\n",
    "    top_pixels = []\n",
    "    for i in range(n):\n",
    "        idx = sorted_indices[i]\n",
    "        row, col = divmod(idx, num_cols)\n",
    "        top_pixels.append((row, col))\n",
    "\n",
    "    return top_pixels\n",
    "\n",
    "\"\"\"##### ***Insertion***\"\"\"\n",
    "\n",
    "def update_instance_with_pixels(current_instance, original_instance, x,y):\n",
    "    \"\"\"\n",
    "    Aggiorna l'immagine inserendo i pixel pi√π importanti.\n",
    "\n",
    "    :param current_instance: Istanza corrente.\n",
    "    :param original_instance: Istanza originale.\n",
    "    :param x: coordinata x del pixel da inserire\n",
    "    :param y: coordinata y del pixel da inserire\n",
    "    :return: Istanza aggiornata con il superpixel.\n",
    "    \"\"\"\n",
    "    new_current_instance = current_instance.copy()\n",
    "    new_current_instance[:, x, y, 0] = original_instance[:, x, y, 0]\n",
    "\n",
    "    return new_current_instance\n",
    "\n",
    "\n",
    "def insertion(model, original_instance, x3_instance, sorted_per_importance_pixels_index, initial_blurred_instance, original_prediction):\n",
    "    \"\"\"\n",
    "    Calcola la metrica di inserimento per una spiegazione data.\n",
    "\n",
    "    :param model: Black-box.\n",
    "    :param original_instance: Istanza originale.\n",
    "    :param sorted_per_importance_pixels_index: Lista di liste di tutti i superpixel per importanza\n",
    "    :param initial_blurred_images: Immagine iniziale con tutti i pixel a zero.\n",
    "    :return: Lista degli errori ad ogni passo di inserimento.\n",
    "    \"\"\"\n",
    "\n",
    "    # Lista per memorizzare le istanze a cui aggiungo pixel mano a mano. Inizializzata con istanza iniziale blurrata\n",
    "    insertion_images = [initial_blurred_instance]\n",
    "\n",
    "    # Predizione sull'immagine iniziale (tutti i pixel a zero)\n",
    "    I_prime = copy.deepcopy(initial_blurred_instance)\n",
    "\n",
    "    # Aggiungere gradualmente i pixel (per ogni frame) pi√π importanti. Ottengo una lista con tutte le img con i pixel aggiunti in maniera graduale\n",
    "    for x,y in sorted_per_importance_pixels_index:\n",
    "        I_prime = update_instance_with_pixels(I_prime, original_instance, x,y)\n",
    "        insertion_images.append(I_prime)\n",
    "\n",
    "    insertion_images = [img.astype(np.float32) for img in insertion_images]\n",
    "    # Calcolo le predizioni sulle istanze a cui ho aggiunto i pixel in maniera graduale\n",
    "    new_predictions = ensemble_predict(model, insertion_images, x3_instance)\n",
    "    # Rispetto ad ogni suddetta predizione, calcolo il MSE rispetto la pred sull'istanza originaria (come da test-set). Ignora la prima che √® sull'img blurrata originale\n",
    "    errors = [mean_squared_error(original_prediction, masked_pred) for masked_pred in new_predictions[1:]]\n",
    "\n",
    "    initial_error = mean_squared_error(original_prediction, new_predictions[0])\n",
    "    print(f\"Initial Prediction with Blurred Instance. Prediction: {new_predictions[0]}, error: {initial_error}\")\n",
    "    only_inserted_pixel_new_predictions = new_predictions[1:]\n",
    "\n",
    "    for nr_pixel, error in enumerate(errors):\n",
    "      print(f\"Inserted Pixel: {sorted_per_importance_pixels_index[nr_pixel]}. Prediction: {only_inserted_pixel_new_predictions[nr_pixel]}, error: {error}\")\n",
    "\n",
    "    total_errors = [initial_error] + errors # Errore iniziale + errori su tutti i pixel inseriti\n",
    "\n",
    "    # # Nuovo asse X\n",
    "    x = np.linspace(0, 1, len(total_errors))\n",
    "    # Calcolo dell'AUC con il nuovo asse x\n",
    "    auc = calculate_auc(x, total_errors)\n",
    "    print(f\"Area under the curve (AUC): {auc}\")\n",
    "    return total_errors,auc\n",
    "\n",
    "\n",
    "def plot_insertion_error_mean_curve(errors_insertion_all_dataset):\n",
    "    # Calcolo dell'errore medio per ogni numero di superpixel inseriti\n",
    "    mean_errors_for_insertion_vott = np.mean(errors_insertion_all_dataset, axis=0)\n",
    "\n",
    "    # Array x per il numero di superpixel inseriti\n",
    "    x = np.arange(0, len(mean_errors_for_insertion_vott))  # Array dinamico basato sulla lunghezza dei dati\n",
    "    auc = calculate_auc(x, mean_errors_for_insertion_vott)\n",
    "\n",
    "    # Creazione del grafico\n",
    "    plt.plot(x, mean_errors_for_insertion_vott, label='Error Curve')\n",
    "\n",
    "    # Pallini blu sui punti della curva\n",
    "    plt.scatter(x, mean_errors_for_insertion_vott, color='blue', zorder=3)\n",
    "\n",
    "    # Area sotto la curva (AUC)\n",
    "    plt.fill_between(x, mean_errors_for_insertion_vott, color='skyblue', alpha=0.4)\n",
    "\n",
    "    # Etichette degli assi\n",
    "    plt.xlabel('Nr of SuperPixels inserted')\n",
    "    plt.ylabel('Mean Squared Error')\n",
    "\n",
    "    # Griglia e stile\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "    # Titolo e legenda\n",
    "    plt.title('Mean Insertion Metric Curve')\n",
    "    plt.legend()\n",
    "\n",
    "    # Visualizzazione del grafico\n",
    "    plt.show()\n",
    "\n",
    "    return auc,mean_errors_for_insertion_vott\n",
    "\n",
    "\"\"\"##### ***Deletion***\"\"\"\n",
    "\n",
    "\n",
    "def update_image_by_removing_pixels(current_instance, x, y, std_zero_value=-0.6486319166678826):\n",
    "    \"\"\"\n",
    "    Aggiorna l'immagine rimuovendo i pixel x,y indicati.\n",
    "\n",
    "    :param current_instance: istanza corrente.\n",
    "    :param x: coordinata x del pixel da rimuovere\n",
    "    :param y: coordinata y del pixel da rimuovere\n",
    "    :return: Istanza aggiornata con x,y rimossi su tutti time-step.\n",
    "    \"\"\"\n",
    "    new_instance = copy.deepcopy(current_instance)\n",
    "    new_instance[:, x, y, 0] = std_zero_value # Imposta i pixel a zero normalizzato per Prec\n",
    "    return new_instance\n",
    "\n",
    "def deletion(models, original_instance, x3_instance, sorted_per_importance_pixels_index, original_prediction):\n",
    "    \"\"\"\n",
    "    Calcola la metrica di rimozione per una spiegazione data.\n",
    "\n",
    "    :param models: Lista di modelli pre-addestrati.\n",
    "    :param original_instance: Immagine originale.\n",
    "    :param x3_instance: Codifica one-hot per la previsione.\n",
    "    :param sorted_per_importance_pixels_index: Indici dei pixel in ordine di importanza.\n",
    "    :return: Lista degli errori, auc ad ogni passo di rimozione.\n",
    "    \"\"\"\n",
    "    # Lista per memorizzare le img a cui elimino gradualmente i pixels (per ogni time-step)\n",
    "    deletion_images = []\n",
    "\n",
    "    # Inizializzazione\n",
    "    I_prime = copy.deepcopy(original_instance)\n",
    "\n",
    "    # Aggiungere gradualmente i pixel (per ogni frame) pi√π importanti. Ottengo una lista con tutte le img con i pixel rimossi\n",
    "    for x, y in sorted_per_importance_pixels_index:\n",
    "        I_prime = update_image_by_removing_pixels(I_prime, x, y)\n",
    "        deletion_images.append(I_prime)\n",
    "\n",
    "    # Calcolo della predizione su tutte le img a cui ho rimosso gradualmente i pixel\n",
    "    new_predictions = ensemble_predict(models, deletion_images, x3_instance)\n",
    "    # Calcolo del mse rispetto la predizione originale\n",
    "    errors = [mean_squared_error(original_prediction, masked_pred) for masked_pred in new_predictions]\n",
    "\n",
    "    initial_error = 0.0\n",
    "    print(f\"Initial Prediction with Original Images, prediction: {original_prediction}, error: {initial_error}\")\n",
    "    for nr_pixel, error in enumerate(errors):\n",
    "        print(f\"Removed pixel {sorted_per_importance_pixels_index[nr_pixel]}, new prediction: {new_predictions[nr_pixel]}, error: {error}\")\n",
    "\n",
    "    total_errors = [initial_error] + errors  # Errore iniziale + errori su tutti i pixel rimossi\n",
    "\n",
    "    # Normalizzare la frazione di pixel rimossi\n",
    "    x = np.linspace(0, 1, len(total_errors))\n",
    "    # Calcolo dell'AUC\n",
    "    auc = calculate_auc(x, total_errors)\n",
    "\n",
    "    print(f\"Area under the curve (AUC): {auc}\")\n",
    "    return total_errors, auc\n",
    "\n",
    "def plot_deletion_error_mean_curve(only_errors_deletion_vott):\n",
    "    \"\"\"\n",
    "    Funzione che calcola la curva di errore media per la rimozione dei superpixel,\n",
    "    calcola l'area sotto la curva (AUC), e visualizza il grafico con l'errore e l'AUC.\n",
    "\n",
    "    :param only_errors_deletion_vott: Array di errori per numero di superpixel rimossi.\n",
    "    :return: AUC e la media degli errori per i superpixel rimossi.\n",
    "    \"\"\"\n",
    "    # Calcola l'errore medio per ogni numero di superpixel rimossi\n",
    "    mean_errors_for_deletion_vott = np.mean(only_errors_deletion_vott, axis=0)\n",
    "\n",
    "    # Array x per il numero di superpixel rimossi\n",
    "    x = np.arange(0, len(mean_errors_for_deletion_vott))  # Array dinamico basato sulla lunghezza dei dati\n",
    "\n",
    "    # Calcola l'AUC utilizzando il metodo del trapezio\n",
    "    auc = calculate_auc(x, mean_errors_for_deletion_vott)\n",
    "\n",
    "    # Creazione del grafico\n",
    "    plt.plot(x, mean_errors_for_deletion_vott, label='Error Curve')\n",
    "\n",
    "    # Pallini rossi sui punti della curva\n",
    "    plt.scatter(x, mean_errors_for_deletion_vott, color='red', zorder=3)\n",
    "\n",
    "    # Area sotto la curva (AUC)\n",
    "    plt.fill_between(x, mean_errors_for_deletion_vott, color='lightcoral', alpha=0.4)\n",
    "\n",
    "    # Etichette degli assi\n",
    "    plt.xlabel('Nr of SuperPixels removed')\n",
    "    plt.ylabel('Mean Squared Error')\n",
    "\n",
    "    # Griglia e stile\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "    # Titolo e legenda\n",
    "    plt.title('Deletion Mean Metric Curve')\n",
    "    plt.legend()\n",
    "\n",
    "    # Visualizzazione del grafico\n",
    "    plt.show()\n",
    "\n",
    "    # Restituisci l'AUC e la media degli errori\n",
    "    return auc, mean_errors_for_deletion_vott\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vott_lstm_models_loaded' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"### ***Experiments***\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m##### Sigmoide\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m channel_prec \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 7\u001b[0m models \u001b[38;5;241m=\u001b[39m \u001b[43mvott_lstm_models_loaded\u001b[49m\n\u001b[1;32m      8\u001b[0m seed \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m42\u001b[39m\n\u001b[1;32m      9\u001b[0m T,H,W,C \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m104\u001b[39m,\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m3\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vott_lstm_models_loaded' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"### ***Experiments***\n",
    "\n",
    "##### Sigmoide\n",
    "\"\"\"\n",
    "\n",
    "channel_prec = 0\n",
    "models = vott_lstm_models_loaded\n",
    "seed = 42\n",
    "T,H,W,C = (104,5,8,3)\n",
    "std_zero_value = -0.6486319166678826\n",
    "\n",
    "N = 1000\n",
    "\n",
    "h_w_values = [(2,2),(2,3),(2,4)]\n",
    "p_values = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "results_setups = []\n",
    "\n",
    "len_test_set = len(vottignasco_test_image)\n",
    "\n",
    "for h,w in h_w_values:\n",
    "    #print(f\"############################## Setup h,w: ({h},{w}) ##############################\")\n",
    "    for p in p_values:\n",
    "        # Conserva tutte le sal_maps per tutto il Test-Set\n",
    "        saliency_maps = np.zeros((len_test_set,H,W))\n",
    "        # Errori e AUC Insertion/Deletion tutto il Test-Set\n",
    "        errors_insertion_all_testset = np.zeros((len_test_set, H*W+1))\n",
    "        auc_insertion_all_testset    = np.zeros((len_test_set, 1))\n",
    "        errors_deletion_all_testset  = np.zeros((len_test_set, H*W+1))\n",
    "        auc_deletion_all_testset     = np.zeros((len_test_set, 1))\n",
    "\n",
    "        p_str = str(p).replace(\".\", \"\")\n",
    "        param_combination = f\"h{h}_w{w}_p{p_str}\"\n",
    "        print(f\"############################## Setup - > parameters combination: {param_combination} ##############################\")   \n",
    "        \n",
    "        for nr_instance,_ in enumerate(vottignasco_test_image):\n",
    "            saliency_map_i = rise_spatial_explain_sigmoide(nr_instance, vottignasco_test_image, vottignasco_test_OHE, models, channel_prec,\n",
    "                                                     N, generate_rise_masks_2d, seed, multiplicative_uniform_noise_onechannel, calculate_saliency_map_ev_masks, h=h, w=w, p=p)\n",
    "            \n",
    "            #plot_frame(saliency_map_i, cmap=\"PuBu\")\n",
    "            \n",
    "            \n",
    "            saliency_maps[nr_instance] = saliency_map_i\n",
    "\n",
    "            instance    = copy.deepcopy(vottignasco_test_image[nr_instance])\n",
    "            x3_instance = copy.deepcopy(vottignasco_test_OHE[nr_instance])\n",
    "\n",
    "            # Insertion\n",
    "            # Video blurrato da cui partire per l'Insertion. Tutti i pixel di Prec su std_zero_value\n",
    "            initial_blurred_instance = copy.deepcopy(instance)\n",
    "            initial_blurred_instance[:,:,:,channel_prec] = std_zero_value\n",
    "\n",
    "            all_important_pixels = get_top_n_pixels(saliency_map_i, instance.shape[1]*instance.shape[2])\n",
    "\n",
    "            original_instance = copy.deepcopy(instance)\n",
    "            original_prediction = ensemble_predict(models, original_instance, x3_instance)\n",
    "            print(f\"Original Prediction: {original_prediction}\")\n",
    "\n",
    "            errors_insertion,auc_insertion = insertion(models, original_instance, x3_instance, all_important_pixels, initial_blurred_instance, original_prediction)\n",
    "            print(f\"Errors Insertion: {errors_insertion}\")\n",
    "            print(f\"AUC Insertion: {auc_insertion}\")\n",
    "\n",
    "            for nr_error, error in enumerate(errors_insertion):\n",
    "                errors_insertion_all_testset[nr_instance][nr_error] = error\n",
    "            \n",
    "            auc_insertion_all_testset[nr_instance] = auc_insertion\n",
    "\n",
    "            # Deletion\n",
    "            errors_deletion,auc_deletion = deletion(models, original_instance, x3_instance, all_important_pixels, original_prediction)\n",
    "            print(f\"Errors Deletion: {errors_deletion}\")\n",
    "            print(f\"AUC Deletion: {auc_deletion}\")\n",
    "\n",
    "            for nr_error, error in enumerate(errors_deletion):\n",
    "                errors_deletion_all_testset[nr_instance][nr_error] = error\n",
    "            \n",
    "            auc_deletion_all_testset[nr_instance] = auc_deletion\n",
    "\n",
    "            print(f\"#################################### END for all Instance in Test-Set for {param_combination} ####################################\")\n",
    "\n",
    "            result = {\n",
    "                \"saliency_maps\": saliency_maps,\n",
    "                \"errors_insertion\": errors_insertion_all_testset,\n",
    "                \"auc_insertion\": auc_insertion_all_testset,\n",
    "                \"errors_deletion\": errors_deletion_all_testset,\n",
    "                \"auc_deletion\": auc_deletion_all_testset,\n",
    "                \"parameters_comb\": param_combination\n",
    "            }\n",
    "\n",
    "            path_to_save_results = os.path.join(work_path, f\"Water_Resources/rise-video/XAI/spatial/results/rise_original_multiplicative_norm_zero/results_setup_new_h_w_inv_sigmoide.pkl\")\n",
    "\n",
    "            # Salvataggio della lista results in un file pickle\n",
    "            with open(path_to_save_results, 'wb') as f:\n",
    "                pickle.dump(results_setups, f)\n",
    "\n",
    "print(\"############################# END FOR ALL SETUPS ##########################################################################\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
